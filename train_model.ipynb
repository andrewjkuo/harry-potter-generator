{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "from pickle import dump, load\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "from keras.optimizers import Adadelta\n",
    "\n",
    "from txt2sequence import convert_to_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Text to Sequences\n",
    "N.B. If 'texts/hp_sequences.txt' has already been generated you don't need to run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing books\n",
      "cleaning text\n",
      "removing infrequent words\n",
      "saving sequences to file\n"
     ]
    }
   ],
   "source": [
    "convert_to_sequences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# load\n",
    "in_filename = 'texts/hp_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:, :-1], sequences[:, -1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 64)            528832    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8263)              1065927   \n",
      "=================================================================\n",
      "Total params: 1,841,671\n",
      "Trainable params: 1,841,671\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 64, input_length=seq_length))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "optimizer = Adadelta()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "# fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "1176639/1176639 [==============================] - 2301s 2ms/step - loss: 6.0407 - acc: 0.0990\n",
      "Epoch 2/100\n",
      "1176639/1176639 [==============================] - 2370s 2ms/step - loss: 5.4109 - acc: 0.1503\n",
      "Epoch 3/100\n",
      "1176639/1176639 [==============================] - 2341s 2ms/step - loss: 5.2561 - acc: 0.1661\n",
      "Epoch 4/100\n",
      "1176639/1176639 [==============================] - 2226s 2ms/step - loss: 5.1843 - acc: 0.1745\n",
      "Epoch 5/100\n",
      "1176639/1176639 [==============================] - 2317s 2ms/step - loss: 5.1356 - acc: 0.1811\n",
      "Epoch 6/100\n",
      "1176639/1176639 [==============================] - 2332s 2ms/step - loss: 5.0987 - acc: 0.1867\n",
      "Epoch 7/100\n",
      "1176639/1176639 [==============================] - 2330s 2ms/step - loss: 5.0686 - acc: 0.1910\n",
      "Epoch 8/100\n",
      "1176639/1176639 [==============================] - 2333s 2ms/step - loss: 5.0479 - acc: 0.1944\n",
      "Epoch 9/100\n",
      "1176639/1176639 [==============================] - 2335s 2ms/step - loss: 5.0269 - acc: 0.1980\n",
      "Epoch 10/100\n",
      "1176639/1176639 [==============================] - 2337s 2ms/step - loss: 5.0091 - acc: 0.2013\n",
      "Epoch 11/100\n",
      "1176639/1176639 [==============================] - 2333s 2ms/step - loss: 5.0002 - acc: 0.2040\n",
      "Epoch 12/100\n",
      "1176639/1176639 [==============================] - 2334s 2ms/step - loss: 4.9853 - acc: 0.2063\n",
      "Epoch 13/100\n",
      "1176639/1176639 [==============================] - 2328s 2ms/step - loss: 4.9727 - acc: 0.2086\n",
      "Epoch 14/100\n",
      "1176639/1176639 [==============================] - 2332s 2ms/step - loss: 4.9609 - acc: 0.2109\n",
      "Epoch 15/100\n",
      "1176639/1176639 [==============================] - 2339s 2ms/step - loss: 4.9524 - acc: 0.2126\n",
      "Epoch 16/100\n",
      "1176639/1176639 [==============================] - 2336s 2ms/step - loss: 4.9365 - acc: 0.2145\n",
      "Epoch 17/100\n",
      "1176639/1176639 [==============================] - 2333s 2ms/step - loss: 4.9315 - acc: 0.2161\n",
      "Epoch 18/100\n",
      "1176639/1176639 [==============================] - 2331s 2ms/step - loss: 4.9217 - acc: 0.2178\n",
      "Epoch 19/100\n",
      "1176639/1176639 [==============================] - 2337s 2ms/step - loss: 4.9105 - acc: 0.2192\n",
      "Epoch 20/100\n",
      "1176639/1176639 [==============================] - 2338s 2ms/step - loss: 4.9068 - acc: 0.2203\n",
      "Epoch 21/100\n",
      "1176639/1176639 [==============================] - 2254s 2ms/step - loss: 4.9014 - acc: 0.2216\n",
      "Epoch 22/100\n",
      "1176639/1176639 [==============================] - 2245s 2ms/step - loss: 4.8902 - acc: 0.2228\n",
      "Epoch 23/100\n",
      "1176639/1176639 [==============================] - 2230s 2ms/step - loss: 4.8851 - acc: 0.2241\n",
      "Epoch 24/100\n",
      "1176639/1176639 [==============================] - 2233s 2ms/step - loss: 4.8826 - acc: 0.2251\n",
      "Epoch 25/100\n",
      "1176639/1176639 [==============================] - 2233s 2ms/step - loss: 4.8886 - acc: 0.2258\n",
      "Epoch 26/100\n",
      "1176639/1176639 [==============================] - 2224s 2ms/step - loss: 4.8844 - acc: 0.2264\n",
      "Epoch 27/100\n",
      "1176639/1176639 [==============================] - 2230s 2ms/step - loss: 4.8734 - acc: 0.2274\n",
      "Epoch 28/100\n",
      "1176639/1176639 [==============================] - 2237s 2ms/step - loss: 4.8688 - acc: 0.2285\n",
      "Epoch 29/100\n",
      "1176639/1176639 [==============================] - 2238s 2ms/step - loss: 4.8690 - acc: 0.2292\n",
      "Epoch 30/100\n",
      "1176639/1176639 [==============================] - 2238s 2ms/step - loss: 4.8708 - acc: 0.2296\n",
      "Epoch 31/100\n",
      "1176639/1176639 [==============================] - 2242s 2ms/step - loss: 4.8713 - acc: 0.2305\n",
      "Epoch 32/100\n",
      "1176639/1176639 [==============================] - 2236s 2ms/step - loss: 4.8700 - acc: 0.2307\n",
      "Epoch 33/100\n",
      "1176639/1176639 [==============================] - 2242s 2ms/step - loss: 4.8641 - acc: 0.2314\n",
      "Epoch 34/100\n",
      "1176639/1176639 [==============================] - 2269s 2ms/step - loss: 4.8489 - acc: 0.2323\n",
      "Epoch 35/100\n",
      "1176639/1176639 [==============================] - 2123s 2ms/step - loss: 4.8428 - acc: 0.2331\n",
      "Epoch 36/100\n",
      "1176639/1176639 [==============================] - 2204s 2ms/step - loss: 4.8334 - acc: 0.2339\n",
      "Epoch 37/100\n",
      "1176639/1176639 [==============================] - 2214s 2ms/step - loss: 4.8367 - acc: 0.2346\n",
      "Epoch 38/100\n",
      "1176639/1176639 [==============================] - 2240s 2ms/step - loss: 4.8377 - acc: 0.2349\n",
      "Epoch 39/100\n",
      "1176639/1176639 [==============================] - 2237s 2ms/step - loss: 4.8409 - acc: 0.2353\n",
      "Epoch 40/100\n",
      "1176639/1176639 [==============================] - 2218s 2ms/step - loss: 4.8390 - acc: 0.2355\n",
      "Epoch 41/100\n",
      "1176639/1176639 [==============================] - 2126s 2ms/step - loss: 4.8406 - acc: 0.2360\n",
      "Epoch 42/100\n",
      "1176639/1176639 [==============================] - 2160s 2ms/step - loss: 4.8431 - acc: 0.2362\n",
      "Epoch 43/100\n",
      "1176639/1176639 [==============================] - 2209s 2ms/step - loss: 4.8477 - acc: 0.2364\n",
      "Epoch 44/100\n",
      "1176639/1176639 [==============================] - 2226s 2ms/step - loss: 4.8550 - acc: 0.2364\n",
      "Epoch 45/100\n",
      "1176639/1176639 [==============================] - 2203s 2ms/step - loss: 4.8512 - acc: 0.2368\n",
      "Epoch 46/100\n",
      "1176639/1176639 [==============================] - 2232s 2ms/step - loss: 4.8504 - acc: 0.2372\n",
      "Epoch 47/100\n",
      "1176639/1176639 [==============================] - 2202s 2ms/step - loss: 4.8482 - acc: 0.2376\n",
      "Epoch 48/100\n",
      "1176639/1176639 [==============================] - 2213s 2ms/step - loss: 4.8479 - acc: 0.2380\n",
      "Epoch 49/100\n",
      "1176639/1176639 [==============================] - 2205s 2ms/step - loss: 4.8526 - acc: 0.2385\n",
      "Epoch 50/100\n",
      "1176639/1176639 [==============================] - 2230s 2ms/step - loss: 4.8560 - acc: 0.2385\n",
      "Epoch 51/100\n",
      "1176639/1176639 [==============================] - 2232s 2ms/step - loss: 4.8554 - acc: 0.2390\n",
      "Epoch 52/100\n",
      "1176639/1176639 [==============================] - 2227s 2ms/step - loss: 4.8556 - acc: 0.2393\n",
      "Epoch 53/100\n",
      "1176639/1176639 [==============================] - 2231s 2ms/step - loss: 4.8620 - acc: 0.2394\n",
      "Epoch 54/100\n",
      "1176639/1176639 [==============================] - 2209s 2ms/step - loss: 4.8600 - acc: 0.2393\n",
      "Epoch 55/100\n",
      "1176639/1176639 [==============================] - 2232s 2ms/step - loss: 4.8544 - acc: 0.2399\n",
      "Epoch 56/100\n",
      "1176639/1176639 [==============================] - 2194s 2ms/step - loss: 4.8579 - acc: 0.2399\n",
      "Epoch 57/100\n",
      "1176639/1176639 [==============================] - 2125s 2ms/step - loss: 4.8540 - acc: 0.2402\n",
      "Epoch 58/100\n",
      "1176639/1176639 [==============================] - 2163s 2ms/step - loss: 4.8618 - acc: 0.2404\n",
      "Epoch 59/100\n",
      "1176639/1176639 [==============================] - 2231s 2ms/step - loss: 4.8649 - acc: 0.2402\n",
      "Epoch 60/100\n",
      "1176639/1176639 [==============================] - 2201s 2ms/step - loss: 4.8690 - acc: 0.2404\n",
      "Epoch 61/100\n",
      "1176639/1176639 [==============================] - 2201s 2ms/step - loss: 4.8737 - acc: 0.2405\n",
      "Epoch 62/100\n",
      "1176639/1176639 [==============================] - 2230s 2ms/step - loss: 4.8743 - acc: 0.2409\n",
      "Epoch 63/100\n",
      "1176639/1176639 [==============================] - 2179s 2ms/step - loss: 4.8799 - acc: 0.2411\n",
      "Epoch 64/100\n",
      "1176639/1176639 [==============================] - 2122s 2ms/step - loss: 4.8828 - acc: 0.2410\n",
      "Epoch 65/100\n",
      "1176639/1176639 [==============================] - 2140s 2ms/step - loss: 4.8844 - acc: 0.2413\n",
      "Epoch 66/100\n",
      "1176639/1176639 [==============================] - 2202s 2ms/step - loss: 4.8881 - acc: 0.2410\n",
      "Epoch 67/100\n",
      "1176639/1176639 [==============================] - 2126s 2ms/step - loss: 4.8885 - acc: 0.2417\n",
      "Epoch 68/100\n",
      "1176639/1176639 [==============================] - 2117s 2ms/step - loss: 4.8925 - acc: 0.2416\n",
      "Epoch 69/100\n",
      "1176639/1176639 [==============================] - 2186s 2ms/step - loss: 4.8983 - acc: 0.2415\n",
      "Epoch 70/100\n",
      "1176639/1176639 [==============================] - 2220s 2ms/step - loss: 4.9040 - acc: 0.2413\n",
      "Epoch 71/100\n",
      "1176639/1176639 [==============================] - 2197s 2ms/step - loss: 4.9116 - acc: 0.2417\n",
      "Epoch 72/100\n",
      "1176639/1176639 [==============================] - 2196s 2ms/step - loss: 4.9102 - acc: 0.2417\n",
      "Epoch 73/100\n",
      "1176639/1176639 [==============================] - 2193s 2ms/step - loss: 4.9177 - acc: 0.2414\n",
      "Epoch 74/100\n",
      "1176639/1176639 [==============================] - 2194s 2ms/step - loss: 4.9180 - acc: 0.2417\n",
      "Epoch 75/100\n",
      "1176639/1176639 [==============================] - 2193s 2ms/step - loss: 4.9171 - acc: 0.2418\n",
      "Epoch 76/100\n",
      "1176639/1176639 [==============================] - 2194s 2ms/step - loss: 4.9196 - acc: 0.2420\n",
      "Epoch 77/100\n",
      "1176639/1176639 [==============================] - 2198s 2ms/step - loss: 4.9222 - acc: 0.2419\n",
      "Epoch 78/100\n",
      "1176639/1176639 [==============================] - 2141s 2ms/step - loss: 4.9239 - acc: 0.2424\n",
      "Epoch 79/100\n",
      "1176639/1176639 [==============================] - 2088s 2ms/step - loss: 4.9263 - acc: 0.2424\n",
      "Epoch 80/100\n",
      "1176639/1176639 [==============================] - 2088s 2ms/step - loss: 4.9303 - acc: 0.2424\n",
      "Epoch 81/100\n",
      "1176639/1176639 [==============================] - 2193s 2ms/step - loss: 4.9315 - acc: 0.2426\n",
      "Epoch 82/100\n",
      "1176639/1176639 [==============================] - 2172s 2ms/step - loss: 4.9343 - acc: 0.2428\n",
      "Epoch 83/100\n",
      "1176639/1176639 [==============================] - 2172s 2ms/step - loss: 4.9419 - acc: 0.2423\n",
      "Epoch 84/100\n",
      "1176639/1176639 [==============================] - 2165s 2ms/step - loss: 4.9496 - acc: 0.2422\n",
      "Epoch 85/100\n",
      "1176639/1176639 [==============================] - 2192s 2ms/step - loss: 4.9493 - acc: 0.2421\n",
      "Epoch 86/100\n",
      "1176639/1176639 [==============================] - 2198s 2ms/step - loss: 4.9547 - acc: 0.2426\n",
      "Epoch 87/100\n",
      "1176639/1176639 [==============================] - 2221s 2ms/step - loss: 4.9584 - acc: 0.2420\n",
      "Epoch 88/100\n",
      "1176639/1176639 [==============================] - 2223s 2ms/step - loss: 4.9599 - acc: 0.2425\n",
      "Epoch 89/100\n",
      "1176639/1176639 [==============================] - 2222s 2ms/step - loss: 4.9549 - acc: 0.2422\n",
      "Epoch 90/100\n",
      "1176639/1176639 [==============================] - 2191s 2ms/step - loss: 4.9514 - acc: 0.2423\n",
      "Epoch 91/100\n",
      "1176639/1176639 [==============================] - 2194s 2ms/step - loss: 4.9532 - acc: 0.2427\n",
      "Epoch 92/100\n",
      "1176639/1176639 [==============================] - 2219s 2ms/step - loss: 4.9586 - acc: 0.2429\n",
      "Epoch 93/100\n",
      "1176639/1176639 [==============================] - 2226s 2ms/step - loss: 4.9603 - acc: 0.2427\n",
      "Epoch 94/100\n",
      "1176639/1176639 [==============================] - 2196s 2ms/step - loss: 4.9599 - acc: 0.2429\n",
      "Epoch 95/100\n",
      "1176639/1176639 [==============================] - 2202s 2ms/step - loss: 4.9730 - acc: 0.2424\n",
      "Epoch 96/100\n",
      "1176639/1176639 [==============================] - 2202s 2ms/step - loss: 4.9756 - acc: 0.2425\n",
      "Epoch 97/100\n",
      "1176639/1176639 [==============================] - 2201s 2ms/step - loss: 4.9817 - acc: 0.2426\n",
      "Epoch 98/100\n",
      "1176639/1176639 [==============================] - 2214s 2ms/step - loss: 4.9796 - acc: 0.2428\n",
      "Epoch 99/100\n",
      "1176639/1176639 [==============================] - 2232s 2ms/step - loss: 4.9852 - acc: 0.2427\n",
      "Epoch 100/100\n",
      "1176639/1176639 [==============================] - 3357s 3ms/step - loss: 4.9841 - acc: 0.2428\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13b801668>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/251019/model.h5')\n",
    "with open('models/251019/model.json', 'w') as json_file:\n",
    "    json_file.write(model.to_json())\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('models/251019/tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
