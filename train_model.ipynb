{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import array\n",
    "from pickle import dump, load\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from txt2sequence import convert_to_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Text to Sequences\n",
    "N.B. If 'texts/hp_sequences.txt' has already been generated you don't need to run the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing books\n",
      "cleaning text\n",
      "removing infrequent words\n",
      "saving sequences to file\n"
     ]
    }
   ],
   "source": [
    "convert_to_sequences()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Language Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# load\n",
    "in_filename = 'texts/hp_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "sequences = array(sequences)\n",
    "X, y = sequences[:, :-1], sequences[:, -1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 64)            528832    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 128)           98816     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8263)              1065927   \n",
      "=================================================================\n",
      "Total params: 1,841,671\n",
      "Trainable params: 1,841,671\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 64, input_length=seq_length))\n",
    "model.add(LSTM(128, return_sequences=True))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model\n",
    "optimizer = Adadelta()\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 1058975 samples, validate on 117664 samples\n",
      "Epoch 1/1000\n",
      "1058975/1058975 [==============================] - 1849s 2ms/step - loss: 6.4580 - accuracy: 0.0706 - val_loss: 6.3575 - val_accuracy: 0.0727\n",
      "Epoch 2/1000\n",
      "1058975/1058975 [==============================] - 1842s 2ms/step - loss: 6.3685 - accuracy: 0.0732 - val_loss: 6.3612 - val_accuracy: 0.0727\n",
      "Epoch 3/1000\n",
      "1058975/1058975 [==============================] - 1871s 2ms/step - loss: 6.2548 - accuracy: 0.0772 - val_loss: 6.1066 - val_accuracy: 0.0857\n",
      "Epoch 4/1000\n",
      "1058975/1058975 [==============================] - 2004s 2ms/step - loss: 5.9305 - accuracy: 0.1014 - val_loss: 5.8148 - val_accuracy: 0.1092\n",
      "Epoch 5/1000\n",
      "1058975/1058975 [==============================] - 2043s 2ms/step - loss: 5.6527 - accuracy: 0.1254 - val_loss: 5.6514 - val_accuracy: 0.1227\n",
      "Epoch 6/1000\n",
      "1058975/1058975 [==============================] - 1953s 2ms/step - loss: 5.4645 - accuracy: 0.1410 - val_loss: 5.4795 - val_accuracy: 0.1391\n",
      "Epoch 7/1000\n",
      "1058975/1058975 [==============================] - 2121s 2ms/step - loss: 5.3344 - accuracy: 0.1495 - val_loss: 5.3814 - val_accuracy: 0.1493\n",
      "Epoch 8/1000\n",
      "1058975/1058975 [==============================] - 2072s 2ms/step - loss: 5.2385 - accuracy: 0.1564 - val_loss: 5.3132 - val_accuracy: 0.1499\n",
      "Epoch 9/1000\n",
      "1058975/1058975 [==============================] - 2031s 2ms/step - loss: 5.1647 - accuracy: 0.1618 - val_loss: 5.3081 - val_accuracy: 0.1525\n",
      "Epoch 10/1000\n",
      "1058975/1058975 [==============================] - 2047s 2ms/step - loss: 5.1023 - accuracy: 0.1663 - val_loss: 5.2544 - val_accuracy: 0.1559\n",
      "Epoch 11/1000\n",
      "1058975/1058975 [==============================] - 2046s 2ms/step - loss: 5.0485 - accuracy: 0.1705 - val_loss: 5.2005 - val_accuracy: 0.1566\n",
      "Epoch 12/1000\n",
      "1058975/1058975 [==============================] - 2072s 2ms/step - loss: 5.0013 - accuracy: 0.1741 - val_loss: 5.1695 - val_accuracy: 0.1648\n",
      "Epoch 13/1000\n",
      "1058975/1058975 [==============================] - 2122s 2ms/step - loss: 4.9591 - accuracy: 0.1775 - val_loss: 5.1488 - val_accuracy: 0.1632\n",
      "Epoch 14/1000\n",
      "1058975/1058975 [==============================] - 2160s 2ms/step - loss: 4.9217 - accuracy: 0.1806 - val_loss: 5.1379 - val_accuracy: 0.1641\n",
      "Epoch 15/1000\n",
      "1058975/1058975 [==============================] - 2141s 2ms/step - loss: 4.8880 - accuracy: 0.1837 - val_loss: 5.1173 - val_accuracy: 0.1629\n",
      "Epoch 16/1000\n",
      "1058975/1058975 [==============================] - 2076s 2ms/step - loss: 4.8574 - accuracy: 0.1860 - val_loss: 5.1037 - val_accuracy: 0.1702\n",
      "Epoch 17/1000\n",
      "1058975/1058975 [==============================] - 2177s 2ms/step - loss: 4.8285 - accuracy: 0.1885 - val_loss: 5.0867 - val_accuracy: 0.1687\n",
      "Epoch 18/1000\n",
      "1058975/1058975 [==============================] - 2357s 2ms/step - loss: 4.8020 - accuracy: 0.1911 - val_loss: 5.0412 - val_accuracy: 0.1726\n",
      "Epoch 19/1000\n",
      "1058975/1058975 [==============================] - 2301s 2ms/step - loss: 4.7775 - accuracy: 0.1930 - val_loss: 5.0538 - val_accuracy: 0.1763\n",
      "Epoch 20/1000\n",
      "1058975/1058975 [==============================] - 2141s 2ms/step - loss: 4.7546 - accuracy: 0.1952 - val_loss: 5.0752 - val_accuracy: 0.1743\n",
      "Epoch 21/1000\n",
      "1058975/1058975 [==============================] - 2185s 2ms/step - loss: 4.7322 - accuracy: 0.1975 - val_loss: 5.0289 - val_accuracy: 0.1756\n",
      "Epoch 22/1000\n",
      "1058975/1058975 [==============================] - 2253s 2ms/step - loss: 4.7117 - accuracy: 0.1992 - val_loss: 5.0090 - val_accuracy: 0.1818\n",
      "Epoch 23/1000\n",
      "1058975/1058975 [==============================] - 2360s 2ms/step - loss: 4.6918 - accuracy: 0.2009 - val_loss: 5.0291 - val_accuracy: 0.1775\n",
      "Epoch 24/1000\n",
      "1058975/1058975 [==============================] - 2310s 2ms/step - loss: 4.6731 - accuracy: 0.2025 - val_loss: 4.9951 - val_accuracy: 0.1810\n",
      "Epoch 25/1000\n",
      "1058975/1058975 [==============================] - 2317s 2ms/step - loss: 4.6544 - accuracy: 0.2041 - val_loss: 4.9779 - val_accuracy: 0.1829\n",
      "Epoch 26/1000\n",
      "1058975/1058975 [==============================] - 2393s 2ms/step - loss: 4.6382 - accuracy: 0.2062 - val_loss: 4.9840 - val_accuracy: 0.1771\n",
      "Epoch 27/1000\n",
      "1058975/1058975 [==============================] - 2574s 2ms/step - loss: 4.6214 - accuracy: 0.2076 - val_loss: 4.9866 - val_accuracy: 0.1808\n",
      "Epoch 28/1000\n",
      "1058975/1058975 [==============================] - 2546s 2ms/step - loss: 4.6053 - accuracy: 0.2086 - val_loss: 5.0078 - val_accuracy: 0.1811\n",
      "Epoch 29/1000\n",
      "1058975/1058975 [==============================] - 2541s 2ms/step - loss: 4.5909 - accuracy: 0.2102 - val_loss: 5.0123 - val_accuracy: 0.1856\n",
      "Epoch 30/1000\n",
      "1058975/1058975 [==============================] - 2123s 2ms/step - loss: 4.5768 - accuracy: 0.2117 - val_loss: 4.9869 - val_accuracy: 0.1772\n",
      "Epoch 31/1000\n",
      "1058975/1058975 [==============================] - 2291s 2ms/step - loss: 4.5619 - accuracy: 0.2132 - val_loss: 4.9822 - val_accuracy: 0.1871\n",
      "Epoch 32/1000\n",
      "1058975/1058975 [==============================] - 2019s 2ms/step - loss: 4.5487 - accuracy: 0.2142 - val_loss: 4.9670 - val_accuracy: 0.1826\n",
      "Epoch 33/1000\n",
      "1058975/1058975 [==============================] - 2113s 2ms/step - loss: 4.5346 - accuracy: 0.2156 - val_loss: 4.9416 - val_accuracy: 0.1897\n",
      "Epoch 34/1000\n",
      "1058975/1058975 [==============================] - 1925s 2ms/step - loss: 4.5219 - accuracy: 0.2168 - val_loss: 5.0068 - val_accuracy: 0.1883\n",
      "Epoch 35/1000\n",
      "1058975/1058975 [==============================] - 1868s 2ms/step - loss: 4.5091 - accuracy: 0.2182 - val_loss: 4.9815 - val_accuracy: 0.1819\n",
      "Epoch 36/1000\n",
      "1058975/1058975 [==============================] - 1804s 2ms/step - loss: 4.4973 - accuracy: 0.2193 - val_loss: 4.9703 - val_accuracy: 0.1910\n",
      "Epoch 37/1000\n",
      "1058975/1058975 [==============================] - 1826s 2ms/step - loss: 4.4884 - accuracy: 0.2201 - val_loss: 4.9712 - val_accuracy: 0.1894\n",
      "Epoch 38/1000\n",
      "1058975/1058975 [==============================] - 1812s 2ms/step - loss: 4.4761 - accuracy: 0.2214 - val_loss: 4.9430 - val_accuracy: 0.1862\n",
      "Epoch 39/1000\n",
      "1058975/1058975 [==============================] - 1785s 2ms/step - loss: 4.4683 - accuracy: 0.2224 - val_loss: 4.9627 - val_accuracy: 0.1916\n",
      "Epoch 40/1000\n",
      "1058975/1058975 [==============================] - 1791s 2ms/step - loss: 4.4562 - accuracy: 0.2234 - val_loss: 4.9647 - val_accuracy: 0.1862\n",
      "Epoch 41/1000\n",
      "1058975/1058975 [==============================] - 1781s 2ms/step - loss: 4.4428 - accuracy: 0.2244 - val_loss: 4.9714 - val_accuracy: 0.1922\n",
      "Epoch 42/1000\n",
      "1058975/1058975 [==============================] - 1844s 2ms/step - loss: 4.4326 - accuracy: 0.2258 - val_loss: 5.0092 - val_accuracy: 0.1908\n",
      "Epoch 43/1000\n",
      "1058975/1058975 [==============================] - 1898s 2ms/step - loss: 4.4232 - accuracy: 0.2269 - val_loss: 4.9698 - val_accuracy: 0.1916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x144c877b8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model.fit(X, y, batch_size=1024, epochs=1000, callbacks=[earlyStopping], validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/080520/model.h5')\n",
    "with open('models/080520/model.json', 'w') as json_file:\n",
    "    json_file.write(model.to_json())\n",
    "# save the tokenizer\n",
    "dump(tokenizer, open('models/080520/tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
