{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import time\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from pickle import dump, load\n",
    "from collections import Counter\n",
    "from tqdm import tqdm_notebook\n",
    "from random import randint\n",
    "from keras.models import load_model, model_from_json, Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "from keras.optimizers import Adadelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "with open('models/251019/model.json', 'r') as json_file:\n",
    "    json_model = json_file.read()\n",
    "\n",
    "model = model_from_json(json_model)\n",
    "model.load_weights('models/251019/model.h5')\n",
    "\n",
    "\n",
    "tokenizer = load(open('models/251019/tokenizer.pkl', 'rb'))\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "unk_ind = tokenizer.word_index['unknownword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "# load cleaned text sequences\n",
    "in_filename = 'texts/hp_sequences.txt'\n",
    "doc = load_doc(in_filename)\n",
    "lines = doc.split('\\n')\n",
    "seq_length = len(lines[0].split()) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "def capitalise(sentence):\n",
    "    return sentence[0].upper() + sentence[1:]\n",
    "\n",
    "def sentence_case(text, punct):\n",
    "    punct = punct + ' '\n",
    "    text = map(capitalise, [x for x in text.split(punct)])\n",
    "    return punct.join(text)\n",
    "\n",
    "def display_txt(text):\n",
    "    first_stop = False\n",
    "    if text[:13] == 'endofsentence':\n",
    "        text = text[13:]\n",
    "        first_stop = True\n",
    "    text = text.replace(' endofsentence ', '. ')\n",
    "    text = text.replace('endofsentence','')\n",
    "    text = sentence_case(text, '.')\n",
    "    text = sentence_case(text, '?')\n",
    "    text = sentence_case(text, '!')\n",
    "    if first_stop:\n",
    "        text = '. ' + text\n",
    "    print(text)\n",
    "    \n",
    "def prepare_txt(text):\n",
    "    text = text.replace('. ', ' endofsentence ')\n",
    "    text = text.lower()\n",
    "    text = ' '.join([word if word in tokenizer.word_index.keys() else 'unknownword' for word in text.split(' ')])\n",
    "    return text\n",
    "\n",
    "def generate_seq(model, tokenizer, seq_length, seed_text, n_words):\n",
    "    result = list()\n",
    "    in_text = seed_text\n",
    "    # generate a fixed number of words\n",
    "    for _ in range(n_words):\n",
    "        # encode the text as integer\n",
    "        encoded = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        # truncate sequences to a fixed length\n",
    "        encoded = pad_sequences([encoded], maxlen=seq_length, truncating='pre')\n",
    "        # predict probabilities for each word\n",
    "        probs = model.predict_proba(encoded, verbose=0)[0]\n",
    "        \n",
    "        # only consider top n best words\n",
    "        n = 5\n",
    "        inds = [x for x in np.argpartition(probs, -n)[-n:] if x != unk_ind]\n",
    "\n",
    "        probs = probs[inds] / probs[inds].sum()\n",
    "\n",
    "        word_ind = np.random.choice(inds, p=probs)\n",
    "        out_word = reverse_word_map[word_ind]\n",
    "        \n",
    "        in_text += ' ' + out_word\n",
    "        result.append(out_word)\n",
    "    return ' '.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mind then. I didnt mean it to happen. Harry said. It was a dream. Can you control what you dream about hermione. If you just learned to apply occlumency but harry was not interested in being told off he wanted to discuss what he had just\n"
     ]
    }
   ],
   "source": [
    "# get random line from the books to use as the seed\n",
    "seed_text = lines[randint(0,len(lines))]\n",
    "display_txt(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seen. But why would he know anything about his first morning he was surprised to see what he was doing. He did not know what he had to do. But there was as he was to discuss the news that they had been expelled and it was a bit more. But if he had to explain he was not surprised to see what he was saying. The first week of gryffindor was saying the whole class was now looking around the end of the school and the students in the castle. They knew what\n"
     ]
    }
   ],
   "source": [
    "generated = generate_seq(model, tokenizer, seq_length, prepare_txt(seed_text), 100)\n",
    "display_txt(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
